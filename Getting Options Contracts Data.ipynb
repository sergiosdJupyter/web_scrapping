{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Options Contracts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "class Process_soup:\n",
    "    def __init__(self, soup):\n",
    "        self.bs = soup\n",
    "        self.price = float(self.get_underlying_price())\n",
    "        self.expdate = self.get_expiration_date()\n",
    "        \n",
    "    \n",
    "    def get_underlying_price(self):\n",
    "        underlying_price = \"\"\n",
    "        keywordIsFound = False\n",
    "        for descendant in self.bs.descendants: # look all tags in document\n",
    "            if descendant.name == \"table\":  # if one tag is a table\n",
    "                for row in descendant.descendants:  # look all the tags under it\n",
    "                    if row.name == 'tr': # if one tag is a table row\n",
    "                        if keywordIsFound:\n",
    "                            underlying_price = row.td.get_text()\n",
    "                            break\n",
    "                        for column in row.children: # loop all the children tags of the row\n",
    "                            if column.name == 'td': # if a children tag is a table column\n",
    "                                if (column.get_text()) == 'Price': # detects the key word, we know the price is next row first column\n",
    "                                    keywordIsFound = True\n",
    "                                    break # stop looking at the columns of the row\n",
    "            if keywordIsFound:\n",
    "                break\n",
    "        return underlying_price\n",
    "    def get_expiration_date(self):\n",
    "        str_date = self.bs.find_all(text=re.compile(\"^Expiry:\"))[0].split()\n",
    "        if str_date:\n",
    "            str_fmt = \"{0} {1} {2}\".format(str_date[1], str_date[2], str_date[3])\n",
    "            return datetime.strptime(str_fmt,\"%b %d, %Y\")\n",
    "        else:\n",
    "            return None\n",
    "    def get_ATM_contract_data(self):\n",
    "        strike = int(self.price * 2)/2\n",
    "        contract = \"{0}P{1:05d}{2:03d}\".format(self.expdate.strftime(\"%y%m%d\"),int(strike),int((strike-int(strike))*1000))\n",
    "        tag=soup.find(onclick=re.compile(contract))\n",
    "        if not tag:\n",
    "            strike=int(self.price)\n",
    "            contract = \"{0}P{1:05d}000\".format(self.expdate.strftime(\"%y%m%d\"),int(strike))\n",
    "            tag=soup.find(onclick=re.compile(contract))\n",
    "            if tag is None:\n",
    "                strike=self.price-self.price%5\n",
    "                contract = \"{0}P{1:05d}000\".format(self.expdate.strftime(\"%y%m%d\"),int(strike))\n",
    "                tag=soup.find(onclick=re.compile(contract))\n",
    "                if tag is None:\n",
    "                    return []\n",
    "        return [contract, strike, tag]\n",
    "    \n",
    "    def get_ATM_option_data(self, tag):\n",
    "        columns = tag.parent.parent.parent.find_all(\"td\")\n",
    "        return {'bid':columns[3].get_text(), 'ask':columns[4].get_text(),\n",
    "                'volume':columns[6].get_text(), 'IV':columns[8].get_text()}\n",
    "    \n",
    "    def print_data(self, contract_data, option_data):\n",
    "        print(\"underlying  : {}\".format(self.price))\n",
    "        print(\"contract    : {}\".format(contract_data[0]))\n",
    "        print(\"strike      : {}\".format(contract_data[1]))\n",
    "        print(\"expiration  : {}\".format(self.expdate.strftime(\"%d %b, %Y\")))\n",
    "        print(\"bid         : {}\".format(option_data['bid']))\n",
    "        print(\"ask         : {}\".format(option_data['ask']))\n",
    "        print(\"volume      : {}\".format(option_data['volume']))\n",
    "        print(\"IV          : {}\".format(option_data['IV']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underlying  : 32.51\n",
      "contract    : 180525P00032500\n",
      "strike      : 32.5\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.01\n",
      "ask         : 0.04\n",
      "volume      : 655\n",
      "IV          : 14.78%\n"
     ]
    }
   ],
   "source": [
    "soup=None\n",
    "with open(\"price_table_T.html\", encoding=\"UTF-8\") as fp:\n",
    "    text = fp.read()\n",
    "soup = BeautifulSoup(text, \"lxml\")\n",
    "#print(soup.prettify())\n",
    "ps = Process_soup(soup)\n",
    "contract_data = ps.get_ATM_contract_data()\n",
    "option_data = ps.get_ATM_option_data(contract_data[2])\n",
    "ps.print_data(contract_data, option_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "\n",
    "class Get_proxies():\n",
    "    \n",
    "    proxies_sources = [\n",
    "    'https://free-proxy-list.net/',\n",
    "    'https://www.us-proxy.org/',\n",
    "    'https://free-proxy-list.net/uk-proxy.html',\n",
    "    'https://free-proxy-list.net/anonymous-proxy.html',\n",
    "    'https://www.sslproxies.org/'\n",
    "    ]\n",
    "\n",
    "    def get_proxies_from_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        parser = fromstring(response.text)\n",
    "        proxies = list()\n",
    "        for i in parser.xpath('//tbody/tr')[:10]:\n",
    "            if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "                #Grabbing IP and corresponding PORT\n",
    "                proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "                proxies.append(proxy)\n",
    "        return proxies\n",
    "    \n",
    "    def get_proxies(self):\n",
    "        proxies = []\n",
    "        for proxies_source in self.proxies_sources:\n",
    "            proxies.extend(self.get_proxies_from_url(proxies_source))\n",
    "        return set(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SPY\n",
      "underlying  : 272.15\n",
      "contract    : 180525P00272000\n",
      "strike      : 272.0\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.04\n",
      "ask         : 0.06\n",
      "volume      : 70866\n",
      "IV          : 6.16%\n",
      "-------------------\n",
      "Error: HTTPSConnectionPool(host='www.optionseducation.org', port=443): Max retries exceeded with url: /quotes.html?quote=HPQ (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden',)))\n",
      "Skipping. Connnection error, server 93.188.162.77:3128\n",
      "-------------------\n",
      "  HPQ\n",
      "underlying  : 21.91\n",
      "contract    : 180525P00021500\n",
      "strike      : 21.5\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.00\n",
      "ask         : 0.01\n",
      "volume      : 0\n",
      "IV          : 44.93%\n",
      "-------------------\n",
      "Error: HTTPSConnectionPool(host='www.optionseducation.org', port=443): Max retries exceeded with url: /quotes.html?quote=HPE (Caused by ConnectTimeoutError(<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000009DDDFD0>, 'Connection to 152.157.119.253 timed out. (connect timeout=5)'))\n",
      "Skipping. Connnection error, server 152.157.119.253:3128\n",
      "-------------------\n",
      "  HPE\n",
      "underlying  : 15.52\n",
      "contract    : 180525P00015500\n",
      "strike      : 15.5\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.00\n",
      "ask         : 0.03\n",
      "volume      : 1421\n",
      "IV          : 21.86%\n",
      "-------------------\n",
      "  ILG\n",
      "underlying  : 34.18\n",
      "contract    : 180615P00034000\n",
      "strike      : 34.0\n",
      "expiration  : 15 Jun, 2018\n",
      "bid         : 0.13\n",
      "ask         : 0.90\n",
      "volume      : 10008\n",
      "IV          : 20.12%\n",
      "-------------------\n",
      "  ON\n",
      "underlying  : 25.85\n",
      "contract    : 180615P00025000\n",
      "strike      : 25\n",
      "expiration  : 15 Jun, 2018\n",
      "bid         : 0.45\n",
      "ask         : 0.55\n",
      "volume      : 103\n",
      "IV          : 35.74%\n",
      "-------------------\n",
      "  T\n",
      "underlying  : 32.51\n",
      "contract    : 180525P00032500\n",
      "strike      : 32.5\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.01\n",
      "ask         : 0.04\n",
      "volume      : 655\n",
      "IV          : 14.78%\n",
      "-------------------\n",
      "  GE\n",
      "underlying  : 14.63\n",
      "contract    : 180525P00014500\n",
      "strike      : 14.5\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.00\n",
      "ask         : 0.01\n",
      "volume      : 5456\n",
      "IV          : 23.60%\n",
      "-------------------\n",
      "  M\n",
      "underlying  : 34.13\n",
      "contract    : 180525P00034000\n",
      "strike      : 34.0\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.00\n",
      "ask         : 0.01\n",
      "volume      : 243\n",
      "IV          : 29.67%\n",
      "-------------------\n",
      "Error: EOF occurred in violation of protocol (_ssl.c:600)\n",
      "Skipping. Connnection error, server 54.209.135.103:3128\n",
      "-------------------\n",
      "  BAC\n",
      "underlying  : 30.16\n",
      "contract    : 180525P00030000\n",
      "strike      : 30.0\n",
      "expiration  : 25 May, 2018\n",
      "bid         : 0.00\n",
      "ask         : 0.01\n",
      "volume      : 9922\n",
      "IV          : 15.64%\n",
      "-------------------\n",
      "  LEG\n",
      "underlying  : 41.63\n",
      "contract    : 180615P00040000\n",
      "strike      : 40.0\n",
      "expiration  : 15 Jun, 2018\n",
      "bid         : 0.20\n",
      "ask         : 0.35\n",
      "volume      : 0\n",
      "IV          : 19.33%\n",
      "-------------------\n",
      "-------END---------\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "import traceback\n",
    "from lxml.html import fromstring\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "tickers = ['SPY','HPQ', 'HPE', 'ILG', 'ON', 'T', 'GE', 'M', 'BAC', 'LEG']\n",
    "proxies = Get_proxies()\n",
    "user_agents = UserAgent()\n",
    "proxy_pool = cycle(proxies.get_proxies())\n",
    "for ticker in tickers:\n",
    "    url = 'https://www.optionseducation.org/quotes.html?quote=' + ticker\n",
    "    for i in proxy_pool:\n",
    "        proxy = next(proxy_pool) # Get a proxy from the pool\n",
    "        user_agent = user_agents.random   # Get a random user agent\n",
    "        headers = {'User-Agent': user_agent}\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            response = requests.get(url, proxies={\"http\": proxy, \"https\": proxy}, headers=headers, timeout = 5)\n",
    "            soup_oic = BeautifulSoup(response.text, \"lxml\")\n",
    "            with open(\"oic_page_\" + ticker + \".html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "                fp.write(response.text)\n",
    "            adress = \"\"\n",
    "            for iframe in soup_oic.find_all('iframe'):\n",
    "                if 'ivolatility' in iframe['src']:\n",
    "                    address = iframe['src']\n",
    "            req = requests.get(address, proxies={\"http\": proxy, \"https\": proxy}, headers=headers, timeout = 5)\n",
    "            soup = BeautifulSoup(req.text, \"lxml\")\n",
    "            with open(\"price_table_\" + ticker + \".html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "                fp.write(req.text)\n",
    "            ps = Process_soup(soup)\n",
    "            print(\"  {}\".format(ticker))\n",
    "            contract_data = ps.get_ATM_contract_data()\n",
    "            option_data = ps.get_ATM_option_data(contract_data[2])\n",
    "            ps.print_data(contract_data, option_data )\n",
    "            print(\"-------------------\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "            #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "            print(\"Error: {}\".format(e))\n",
    "            print(\"Skipping. Connnection error, server {}\".format(proxy))\n",
    "        print(\"-------------------\")\n",
    "print(\"-------END---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:web_scrapping3]",
   "language": "python",
   "name": "conda-env-web_scrapping3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
