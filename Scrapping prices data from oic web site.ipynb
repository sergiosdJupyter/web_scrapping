{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping prices data\n",
    "\n",
    "## Getting the source page\n",
    "\n",
    "Example ticker is HPQ, with page data <https://www.optionseducation.org/quotes.html?quote=HPQ>. This page has been selected since it's well protected agaisnt robots (they donÂ´t like you put your nose on them...)\n",
    "\n",
    "This page offers undelying and options prices along with other information.\n",
    "\n",
    "First focus on the stock price. it can be found in a table below the ticker and market information. If the page is opened with Chrome and then right click on the price, selecting \"Inspeccionar\", a new window will be opened showing the html associated to the price information, in this case the data price is embedded in a table with the following structure:\n",
    "<table cellpadding=\"2\" cellspacing=\"1\" border=\"0\" bgcolor=\"#999999\" width=\"100%\">\n",
    "<tbody><tr bgcolor=\"#cccccc\" align=\"center\" class=\"s1\">\n",
    "<td>Price</td><td>Change&nbsp;(%)</td><td>52&nbsp;wk&nbsp;High</td><td>52&nbsp;wk&nbsp;Low</td><td>Stock Volume&nbsp;\n",
    "\t\t\t\t\t<a href=\"javascript:openHelp(14)\" alt=\"Open Help\"><img src=\"/design/images/ico/question_btn.gif\" width=\"10\" height=\"10\" border=\"0\" alt=\"Open Help\"></a></td><td>Avg. options volume&nbsp;\n",
    "\t\t\t\t\t<a href=\"javascript:openHelp(10222)\" alt=\"Open Help\"><img src=\"/design/images/ico/question_btn.gif\" width=\"10\" height=\"10\" border=\"0\" alt=\"Open Help\"></a></td><td>Avg. options open interest&nbsp;\n",
    "\t\t\t\t\t<a href=\"javascript:openHelp(11)\" alt=\"Open Help\"><img src=\"/design/images/ico/question_btn.gif\" width=\"10\" height=\"10\" border=\"0\" alt=\"Open Help\"></a></td>\n",
    "</tr>\n",
    "<tr bgcolor=\"#eeeeee\" align=\"center\" class=\"s1\">\n",
    "<td>21.91</td><td><nobr><img src=\"/design/images/ico/arrow_bottom.gif\" alt=\"-\" border=\"0\" align=\"absmiddle\" width=\"7\" height=\"9\">&nbsp; \n",
    "\t\t\t\t\t\t-0.01&nbsp;(-0.05%)\n",
    "\t\t\t\t\t</nobr></td><td><nobr>24.65&nbsp;09-Mar</nobr></td><td><nobr>17.19&nbsp;06-Jul</nobr></td><td>7683571</td><td>9,150</td><td>192,050</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "```\n",
    "<table cellpadding=\"2\" cellspacing=\"1\" border=\"0\" bgcolor=\"#999999\" width=\"100%\">\n",
    "    <tbody>\n",
    "        <tr bgcolor=\"#cccccc\" align=\"center\" class=\"s1\">\n",
    "            <td>Price</td>\n",
    "            ...\n",
    "        </tr>\n",
    "        <tr bgcolor=\"#eeeeee\" align=\"center\" class=\"s1\">\n",
    "            <td>21.91</td>\n",
    "            ...\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "```\n",
    "A way of identifying this table in the structure is to look for the \"Price\" string as data of first row and then we know the actual price will be the data of the second row.\n",
    "\n",
    "Let's get the web page and save to a file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "req = requests.get('https://www.optionseducation.org/quotes.html?quote=HPQ')\n",
    "soup_oic = BeautifulSoup(req.text, \"lxml\")\n",
    "with open(\"hpq_oic\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open the hpq.html file we will realize that this table is not indeed in the web page!!.\n",
    "\n",
    "This has happened because the web page is just a loaded page as is indicated by the tag before:\n",
    "\n",
    "```\n",
    "<iframe  src=\"https://oic.ivolatility.com/oic_adv_options.j?cnt=a4fe7416d6719b97decda2b53ef5ac290da77083b4582616&ticker=HPQ\" width=\"100%\" height=\"100%\" style=\"height:800px\" ></iframe>\n",
    "```\n",
    "\n",
    "If we open that web page \n",
    "<https://oic.ivolatility.com/oic_adv_options.j?cnt=a4fe7416d6719b97decda2b53ef5ac290da77083b4582616&ticker=HPQ>\n",
    "we can see that just the table is shown\n",
    "\n",
    "So, this page is the one that we have to collect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = requests.get('https://oic.ivolatility.com/oic_adv_options.j?cnt=a4fe7416d6719b97decda2b53ef5ac290da77083b4582616&ticker=HPQ')\n",
    "soup_table = BeautifulSoup(req.text, \"lxml\")\n",
    "with open(\"hpq_table.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the only issue is the address has metadata embedded, the only way to access to this last page is first access to the optionseducation.org quote page, get the link to the volatility.com page and access it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to get the address information, we must loop over all iframe tags and look for the one which contents the \"ivolatiliy\" string inside the src attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oic.ivolatility.com/oic_adv_options.j?cnt=a4fe7416d6719b9707a161a4aa4ba08e0ed7c2e7d264f4a0&ticker=HPQ\n"
     ]
    }
   ],
   "source": [
    "for iframe in soup_oic.find_all('iframe'):\n",
    "    if 'ivolatility' in iframe['src']:\n",
    "        address = iframe['src']\n",
    "        print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can access to the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = requests.get(address)\n",
    "soup_table = BeautifulSoup(req.text, \"lxml\")\n",
    "with open(\"hpq_table\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the process but now with another ticker, example ON:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oic.ivolatility.com/oic_adv_options.j?cnt=a4fe7416d6719b9738e0600c9dd56ba2af2bf4ed41f31ca7&ticker=ON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ticker = 'ON'\n",
    "url = 'https://www.optionseducation.org/quotes.html?quote=' + ticker\n",
    "req = requests.get(url)\n",
    "soup_oic = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "with open(\"oic_page.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)\n",
    "    \n",
    "for iframe in soup_oic.find_all('iframe'):\n",
    "    if 'ivolatility' in iframe['src']:\n",
    "        address = iframe['src']\n",
    "        print(address)\n",
    "        \n",
    "req = requests.get(address)\n",
    "table = BeautifulSoup(req.text, \"lxml\")\n",
    "with open(\"table.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But still it doesn't work, now the optionseducation.org reports an error. Looking into the inspector it seems that the server detected that we have access not using a browser and qualifies us as a 'robot':\n",
    "```\n",
    "<meta name=\"robots\" content=\"noindex,nofollow\">\n",
    "...\n",
    "<div id=\"error500\">\n",
    "    <p class=\"alert\">We're sorry, there was a problem processing your request (error 500)</p>\n",
    "    <p>Please try again in a few moments and if you continue to have problems, please contact Investor Services at 1-888-OPTIONS.\n",
    "</p>\n",
    "</div>\n",
    "```\n",
    "How is possible that the server detected that we are accessing to the page through python instead of a browser? The answer could be the headers\n",
    "\n",
    "If we access to the page <http://www.reliply.org/tools/requestheaders.php> we can see the headers used by our browser to access to this page, in this case:\n",
    "<table class=\"standard\" cellspacing=\"1\" border=\"0\" cellpadding=\"4\"><tbody><tr><th class=\"standard\" colspan=\"3\"><center><font size=\"+1\"><b>HTTP Request Headers Sent From Your Browser, Plus Explaination</b></font></center></th></tr><tr><td class=\"standard_evenrow\"><center><b>Header Name</b></center></td><td class=\"standard_evenrow\"><center><b><i>Your</i> Header Value</b></center></td><td class=\"standard_evenrow\"><center><b>Explaination</b></center></td></tr><tr><td class=\"standard_oddrow\">Accept Language</td><td class=\"standard_oddrow\">es-ES,es;q=0.9,fr;q=0.8,it;q=0.7</td><td class=\"standard_oddrow\">This is the preferred language of the browser.  Some websites look at this header and change the language of the page according to the accept language.  In many browsers you can change the HTTP Accept Language if you look deep enough in the configuration/settins/options.  Look here for <a href=\"../info/internet/http-accept-lang.html\" title=\"How To Change My HTTP Accept Language\">How To Change My HTTP Accept Language</a></td></tr><tr><td class=\"standard_evenrow\">Accept Encoding</td><td class=\"standard_evenrow\">gzip, deflate</td><td class=\"standard_evenrow\">This tells the webserver if your browser is able to accept compressed webpages.  This can make a big difference in download time and bandwidth usage.  For compressed webpages to be used, the browser must accept them <i>and</i> the webserver must send them.  Look here for an easy way to <a href=\"../info/internt/enable-compressed-webpages.html\" title=\"Enable Gzip Compression On Your Php Pages To Save Bandwith\">Enable Gzip Compression on your Php Webpages</a> (Saves bandwidth and reduces download times).</td></tr><tr><td class=\"standard_oddrow\">User Agent</td><td class=\"standard_oddrow\">Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36</td><td class=\"standard_oddrow\">This identifies the make and model of your browser.  Some websites use this information to indirectly know the capabilities of your browser and change (or tailor) the webpage for your best browsing experience.<br>Some browsers actually allow you to modify the User Agent.  For example the Opera browser Allows you to change its identity to Microsoft Internet Explorer or Mozilla.</td></tr><tr><td class=\"standard_evenrow\">HTTP Connection</td><td class=\"standard_evenrow\"><i>Your browser did not send this request header</i></td><td class=\"standard_evenrow\">Defines the type of connection your browser wants to establish with the webserver.</td></tr><tr><td class=\"standard_oddrow\">Host</td><td class=\"standard_oddrow\">www.reliply.org</td><td class=\"standard_oddrow\">This is the host or domain name that the browser is attempting to connect to.</td></tr><tr><td class=\"standard_oddrow\">Request Method</td><td class=\"standard_oddrow\">GET</td><td class=\"standard_oddrow\">This can be GET, POST, PUT or HEAD.  GET is the \"normal\" method for viewing a webpage. POST can be used to send form data and the HEAD request method is for retrieving only the headers of a page instead of downloading the entire page.</td></tr></tbody></table>\n",
    "\n",
    "So if we access the same page but using the requests package of python and save the page to the headers.html file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('http://www.reliply.org/tools/requestheaders.php')\n",
    "soup_headers = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "with open(\"headers.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the file:\n",
    "<table class=\"standard\" cellspacing=\"1\" border=\"0\" cellpadding=\"4\"><tbody><tr><th class=\"standard\" colspan=\"3\"><center><font size=\"+1\"><b>HTTP Request Headers Sent From Your Browser, Plus Explaination</b></font></center></th></tr><tr><td class=\"standard_evenrow\"><center><b>Header Name</b></center></td><td class=\"standard_evenrow\"><center><b><i>Your</i> Header Value</b></center></td><td class=\"standard_evenrow\"><center><b>Explaination</b></center></td></tr><tr><td class=\"standard_oddrow\">Accept Language</td><td class=\"standard_oddrow\"><i>Your browser did not send this request header</i></td><td class=\"standard_oddrow\">This is the preferred language of the browser.  Some websites look at this header and change the language of the page according to the accept language.  In many browsers you can change the HTTP Accept Language if you look deep enough in the configuration/settins/options.  Look here for <a href=\"../info/internet/http-accept-lang.html\" title=\"How To Change My HTTP Accept Language\">How To Change My HTTP Accept Language</a></td></tr><tr><td class=\"standard_evenrow\">Accept Encoding</td><td class=\"standard_evenrow\">gzip, deflate</td><td class=\"standard_evenrow\">This tells the webserver if your browser is able to accept compressed webpages.  This can make a big difference in download time and bandwidth usage.  For compressed webpages to be used, the browser must accept them <i>and</i> the webserver must send them.  Look here for an easy way to <a href=\"../info/internt/enable-compressed-webpages.html\" title=\"Enable Gzip Compression On Your Php Pages To Save Bandwith\">Enable Gzip Compression on your Php Webpages</a> (Saves bandwidth and reduces download times).</td></tr><tr><td class=\"standard_oddrow\">User Agent</td><td class=\"standard_oddrow\">python-requests/2.14.2</td><td class=\"standard_oddrow\">This identifies the make and model of your browser.  Some websites use this information to indirectly know the capabilities of your browser and change (or tailor) the webpage for your best browsing experience.<br>Some browsers actually allow you to modify the User Agent.  For example the Opera browser Allows you to change its identity to Microsoft Internet Explorer or Mozilla.</td></tr><tr><td class=\"standard_evenrow\">HTTP Connection</td><td class=\"standard_evenrow\"><i>Your browser did not send this request header</i></td><td class=\"standard_evenrow\">Defines the type of connection your browser wants to establish with the webserver.</td></tr><tr><td class=\"standard_oddrow\">Host</td><td class=\"standard_oddrow\">www.reliply.org</td><td class=\"standard_oddrow\">This is the host or domain name that the browser is attempting to connect to.</td></tr><tr><td class=\"standard_oddrow\">Request Method</td><td class=\"standard_oddrow\">GET</td><td class=\"standard_oddrow\">This can be GET, POST, PUT or HEAD.  GET is the \"normal\" method for viewing a webpage. POST can be used to send form data and the HEAD request method is for retrieving only the headers of a page instead of downloading the entire page.</td></tr></tbody></table>\n",
    "\n",
    "Ahaaa! Comparing  the browser headers, we can see there are two headers that are different:\n",
    "- Accept language\n",
    "- User Agent\n",
    "\n",
    "It seems that the important one is the User Agent, when accessing through Chrome and python, the value is:\n",
    "\n",
    "| From   | User Agent value  |\n",
    "|:---|:---:|\n",
    "| Chrome | Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36 |\n",
    "| Python | python-requests/2.14.2 |\n",
    "\n",
    "Likely the server detects this header and deduces that we are accesing through a robot. So the point is to change the header User Agent and use the same as Chrome.\n",
    "\n",
    "Since it's so easy to do that, let's change the value of Accept-Language as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oic.ivolatility.com/oic_adv_options.j?cnt=a4fe7416d6719b97348d213e597fdafad34ce92e7979084f&ticker=HPQ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ticker = 'HPQ'\n",
    "url = 'https://www.optionseducation.org/quotes.html?quote=' + ticker\n",
    "headers = {\n",
    "    'Accept-Language': 'es-ES,es;q=0.9,fr;q=0.8,it;q=0.7',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',\n",
    "}\n",
    "req = requests.get(url, headers=headers)\n",
    "soup_oic = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "with open(\"oic_page.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)\n",
    "    \n",
    "for iframe in soup_oic.find_all('iframe'):\n",
    "    if 'ivolatility' in iframe['src']:\n",
    "        address = iframe['src']\n",
    "        print(address)\n",
    "        \n",
    "req = requests.get(address)\n",
    "table = BeautifulSoup(req.text, \"lxml\")\n",
    "with open(\"table.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works like a piece of cake !!\n",
    "\n",
    "## Getting the underlying price\n",
    "\n",
    "We can see in the html structure that the underlying price is actually the first column of the second row of one of the tables. \n",
    "\n",
    "Since price is just a float and there are a lot of floats in the tables, we must to search first for a positioning keyword. The keyword string selected is 'Price', we know the price number is the row inmediately below 'Price' string and there is not any other 'Price' string before this. So 'Price' keyword is a perfect selection.\n",
    "\n",
    "Therefore first we have to iterate over all the tags of the document, to do that we will use:\n",
    "```\n",
    "for descendant in soup.descendants:\n",
    "```\n",
    "When we detect that one tag is a table:\n",
    "```\n",
    "if descendant.name == \"table\":\n",
    "```\n",
    "then again we will iterate over all descendants of that table, looking for rows.\n",
    "\n",
    "We repeat the process for the rows but now looking for columns, this time we don't need to check all descendants, just the children of each row:\n",
    "```\n",
    "for column in row.children:\n",
    "```\n",
    "\n",
    "then when a column value is our keyword then we know we have been positioned in the right location, the comparison code is:\n",
    "\n",
    "```\n",
    "if (column.get_text()) == 'Price':\n",
    "```\n",
    "\n",
    "when this happens, the boolean keywordIsFound gets marked as True and a break is executed to stop looking for more columns in the first row. Now the external iteration, the one that iterates over all rows of the table, loads the next row (the one that we are interested for) and assign the output variable underlying_price to the text value of the first column of that row:\n",
    "```\n",
    "underlying_price = row.td.get_text()\n",
    "```\n",
    "Finally a sequence of breaks happen and the code print out the underlying price. Awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underlying price = 21.91\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"table.html\", encoding=\"UTF-8\") as fp:\n",
    "    text = fp.read()\n",
    "soup = BeautifulSoup(text, \"lxml\")\n",
    "underlying_price = \"\"\n",
    "keywordIsFound = False\n",
    "for descendant in soup.descendants: # look all tags in document\n",
    "    if descendant.name == \"table\":  # if one tag is a table\n",
    "        for row in descendant.descendants:  # look all the tags under it\n",
    "            if row.name == 'tr': # if one tag is a table row\n",
    "                if keywordIsFound:\n",
    "                    underlying_price = row.td.get_text()\n",
    "                    break\n",
    "                for column in row.children: # loop all the children tags of the row\n",
    "                    if column.name == 'td': # if a children tag is a table column\n",
    "                        if (column.get_text()) == 'Price': # detects the key word, we know the price is next row first column\n",
    "                            keywordIsFound = True\n",
    "                            break # stop looking at the columns of the row\n",
    "    if keywordIsFound:\n",
    "        break\n",
    "print('Underlying price = {}'.format(underlying_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY Underlying Price = 34.18\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ticker = 'SPY'\n",
    "url = 'https://www.optionseducation.org/quotes.html?quote=' + ticker\n",
    "headers = {\n",
    "    'Accept-Language': 'es-ES,es;q=0.9,fr;q=0.8,it;q=0.7',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',\n",
    "}\n",
    "req = requests.get(url, headers=headers)\n",
    "soup_oic = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "with open(\"oic_page.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)\n",
    "    \n",
    "for iframe in soup_oic.find_all('iframe'):\n",
    "    if 'ivolatility' in iframe['src']:\n",
    "        address = iframe['src']       \n",
    "req = requests.get(address)\n",
    "soup = BeautifulSoup(req.text, \"lxml\")\n",
    "with open(\"table.html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "    fp.write(req.text)\n",
    "underlying_price = \"\"\n",
    "keywordIsFound = False\n",
    "for descendant in soup.descendants: # look all tags in document\n",
    "    if descendant.name == \"table\":  # if one tag is a table\n",
    "        for row in descendant.descendants:  # look all the tags under it\n",
    "            if row.name == 'tr': # if one tag is a table row\n",
    "                if keywordIsFound:\n",
    "                    underlying_price = row.td.get_text()\n",
    "                    break\n",
    "                for column in row.children: # loop all the children tags of the row\n",
    "                    if column.name == 'td': # if a children tag is a table column\n",
    "                        if (column.get_text()) == 'Price': # detects the key word, we know the price is next row first column\n",
    "                            keywordIsFound = True\n",
    "                            break # stop looking at the columns of the row\n",
    "    if keywordIsFound:\n",
    "        break\n",
    "print('{0} Underlying Price = {1}'.format(ticker, underlying_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, playing just a bit, I realize that the server continues detecting the access to the page from the script and not by a human, so it got me banned another time.\n",
    "\n",
    "This could happen for several reasons, for example, it's very suspicious that we always access with the same origin IP and/or using the same (fake) browser. The code in the servers to avoid scrapping from robots could be a little sofisticated.\n",
    "\n",
    "There are two possible mechanisms we could apply to the script in order to avoid detection as a robot:\n",
    "- rotate proxies and IP addresses\n",
    "- fake and rotate user agents\n",
    "\n",
    "## Getting your output/public ip address\n",
    "\n",
    "This is your ip adress seen in internet when accessing any server. Let's use the ipify web page:\n",
    "<https://www.ipify.org/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My public IP address is: 95.23.32.230\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "\n",
    "ip = get('https://api.ipify.org').text\n",
    "print('My public IP address is: {}'.format(ip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that if the server sees that there are frequent access using this address, it can deduce that you are a robot and not a human being using a web browser. To avoid this we could try to use proxies (anonymous proxies) in which the IP origin is hidden and the server only can see the address of the proxy itself\n",
    "\n",
    "## Using proxies\n",
    "\n",
    "See the article\n",
    "[How To Rotate Proxies and IP Addresses using Python 3](https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/). Using the script shown in that article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'195.208.172.70:8080', '167.99.188.36:3128', '188.170.222.130:8080', '176.119.112.10:53281', '153.122.54.16:8118', '191.252.100.87:3128', '200.54.194.12:53281', '153.122.52.77:8118', '191.209.72.68:53281'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml.html import fromstring\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "proxies = get_proxies()\n",
    "print(proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, spanning a bit the proxy search and encapsulating everything in a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'46.101.11.131:3128', '207.32.30.20:8080', '159.65.234.60:8080', '181.215.114.144:8123', '89.222.217.185:31588', '35.227.54.242:3128', '147.135.210.114:54566', '23.97.215.153:3128', '191.252.100.68:80', '80.211.189.165:3128', '139.255.57.32:8080', '191.252.100.87:3128', '128.199.254.244:3128', '54.233.85.126:3128', '185.126.201.99:8080', '176.53.37.231:8080', '191.252.195.27:80', '103.214.255.118:3128', '218.148.196.156:808', '187.247.80.56:8080', '117.6.161.118:53281', '152.157.119.253:3128', '54.209.135.103:3128', '41.222.57.164:53281', '178.62.108.246:8118'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "class Get_proxies():\n",
    "    \n",
    "    proxies_sources = [\n",
    "    'https://free-proxy-list.net/',\n",
    "    'https://www.us-proxy.org/',\n",
    "    'https://free-proxy-list.net/uk-proxy.html',\n",
    "    'https://free-proxy-list.net/anonymous-proxy.html',\n",
    "    'https://www.sslproxies.org/'\n",
    "    ]\n",
    "\n",
    "    def get_proxies_from_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        parser = fromstring(response.text)\n",
    "        proxies = list()\n",
    "        for i in parser.xpath('//tbody/tr')[:10]:\n",
    "            if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "                #Grabbing IP and corresponding PORT\n",
    "                proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "                proxies.append(proxy)\n",
    "        return proxies\n",
    "    \n",
    "    def get_proxies(self):\n",
    "        proxies = []\n",
    "        for proxies_source in self.proxies_sources:\n",
    "            proxies.extend(self.get_proxies_from_url(proxies_source))\n",
    "        return set(proxies)\n",
    "\n",
    "my_proxies = Get_proxies()\n",
    "print(my_proxies.get_proxies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the list of free proxies we can use. To see how they \"change\" our IP public adress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1, server 46.101.11.131:3128\n",
      "-----------------> Skipping. Connnection error\n",
      "Request 2, server 185.145.105.83:53281\n",
      "-----------------> Skipping. Connnection error\n",
      "Request 3, server 110.77.189.213:62225\n",
      "{'origin': '110.77.189.213'}\n",
      "Request 4, server 42.104.84.107:8080\n",
      "{'origin': '122.183.242.53'}\n",
      "Request 5, server 35.227.54.242:3128\n",
      "{'origin': '35.227.54.242'}\n",
      "Request 6, server 147.135.210.114:54566\n",
      "{'origin': '147.135.210.114'}\n",
      "Request 7, server 23.97.215.153:3128\n",
      "{'origin': '104.45.31.193'}\n",
      "Request 8, server 54.39.40.100:3128\n",
      "-----------------> Skipping. Connnection error\n",
      "Request 9, server 80.211.189.165:3128\n",
      "{'origin': '80.211.189.165'}\n",
      "Request 10, server 191.252.100.87:3128\n",
      "-----------------> Skipping. Connnection error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "\n",
    "proxies = Get_proxies()\n",
    "proxy_pool = cycle(proxies.get_proxies())\n",
    "\n",
    "url = 'https://httpbin.org/ip'\n",
    "for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "    proxy = next(proxy_pool)\n",
    "    print(\"Request {}, server {}\".format(i,proxy))\n",
    "    try:\n",
    "        response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "        print(response.json())\n",
    "    except:\n",
    "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "        print(\"-----------------> Skipping. Connnection error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the code above to cycle among the proxies. Before that, let's fake and rotate the user agents header\n",
    "\n",
    "## Faking and rotating User Agents\n",
    "\n",
    "Again, the information is extracted from [How to fake and rotate User Agents using Python 3](https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/)\n",
    "\n",
    "One easy way is to install and use the package fake-useragent (in jupyter notebook should be installed through pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n",
      "Request 2\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n",
      "Request 3\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1623.0 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1623.0 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "\n",
    "\n",
    "ua = UserAgent()\n",
    "url = 'https://httpbin.org/user-agent'\n",
    "for i in range(1,4):\n",
    "    user_agent = ua.random\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    print(\"Request {0}\\nUser-Agent Sent:{1}\\nUser Agent Recevied by HTTPBin:\".format(i,user_agent))\n",
    "    print(response.content)\n",
    "    print(\"-------------------\\n\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using proxies and fake user agents at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1\n",
      "User-Agent Sent:\n",
      "Opera/9.80 (X11; Linux i686; U; es-ES) Presto/2.8.131 Version/11.11\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Opera/9.80 (X11; Linux i686; U; es-ES) Presto/2.8.131 Version/11.11\"}\\n'\n",
      "Public IP:\n",
      "{'origin': '46.101.11.131'}\n",
      "-------------------\n",
      "Request 2\n",
      "Skipping. Connnection error, server 185.145.105.83:53281\n",
      "-------------------\n",
      "Request 3\n",
      "Skipping. Connnection error, server 110.77.189.213:62225\n",
      "-------------------\n",
      "Request 4\n",
      "Skipping. Connnection error, server 42.104.84.107:8080\n",
      "-------------------\n",
      "Request 5\n",
      "User-Agent Sent:\n",
      "Mozilla/5.0 (Windows NT 6.1; rv:22.0) Gecko/20130405 Firefox/22.0\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 6.1; rv:22.0) Gecko/20130405 Firefox/22.0\"}\\n'\n",
      "Public IP:\n",
      "{'origin': '35.227.54.242'}\n",
      "-------------------\n",
      "Request 6\n",
      "User-Agent Sent:\n",
      "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0) chromeframe/10.0.648.205\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0) chromeframe/10.0.648.205\"}\\n'\n",
      "Public IP:\n",
      "{'origin': '147.135.210.114'}\n",
      "-------------------\n",
      "Request 7\n",
      "User-Agent Sent:\n",
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36\"}\\n'\n",
      "Public IP:\n",
      "{'origin': '104.45.31.193'}\n",
      "-------------------\n",
      "Request 8\n",
      "Skipping. Connnection error, server 54.39.40.100:3128\n",
      "-------------------\n",
      "Request 9\n",
      "User-Agent Sent:\n",
      "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\"}\\n'\n",
      "Public IP:\n",
      "{'origin': '80.211.189.165'}\n",
      "-------------------\n",
      "Request 10\n",
      "Skipping. Connnection error, server 191.252.100.87:3128\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "from lxml.html import fromstring\n",
    "import time\n",
    "\n",
    "proxies = Get_proxies()\n",
    "proxy_pool = cycle(proxies.get_proxies())\n",
    "url_1 = 'https://httpbin.org/user-agent'\n",
    "url_2 = 'https://httpbin.org/ip'\n",
    "for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "    proxy = next(proxy_pool)\n",
    "    user_agent = ua.random\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    print(\"Request {}\".format(i))\n",
    "    try:\n",
    "        response = requests.get(url_1, proxies={\"http\": proxy, \"https\": proxy}, headers=headers)\n",
    "        print(\"User-Agent Sent:\\n{}\\nUser Agent Recevied by HTTPBin:\".format(user_agent))\n",
    "        print(response.content)\n",
    "        response = requests.get(url_2, proxies={\"http\": proxy, \"https\": proxy}, headers=headers)\n",
    "        print(\"Public IP:\")\n",
    "        print(response.json())       \n",
    "        \n",
    "    except:\n",
    "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "        print(\"Skipping. Connnection error, server {}\".format(proxy))\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Code\n",
    "Now everything is ready to start collecting the prices of different, tickers. \n",
    "\n",
    "First let's encapsulate the soup processing in one class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Process_soup():\n",
    "    def get_underlying_price(self, soup):\n",
    "        underlying_price = \"\"\n",
    "        keywordIsFound = False\n",
    "        for descendant in soup.descendants: # look all tags in document\n",
    "            if descendant.name == \"table\":  # if one tag is a table\n",
    "                for row in descendant.descendants:  # look all the tags under it\n",
    "                    if row.name == 'tr': # if one tag is a table row\n",
    "                        if keywordIsFound:\n",
    "                            underlying_price = row.td.get_text()\n",
    "                            break\n",
    "                        for column in row.children: # loop all the children tags of the row\n",
    "                            if column.name == 'td': # if a children tag is a table column\n",
    "                                if (column.get_text()) == 'Price': # detects the key word, we know the price is next row first column\n",
    "                                    keywordIsFound = True\n",
    "                                    break # stop looking at the columns of the row\n",
    "            if keywordIsFound:\n",
    "                break\n",
    "        return underlying_price\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the same for the get_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "\n",
    "class Get_proxies():\n",
    "    \n",
    "    proxies_sources = [\n",
    "    'https://free-proxy-list.net/',\n",
    "    'https://www.us-proxy.org/',\n",
    "    'https://free-proxy-list.net/uk-proxy.html',\n",
    "    'https://free-proxy-list.net/anonymous-proxy.html',\n",
    "    'https://www.sslproxies.org/'\n",
    "    ]\n",
    "\n",
    "    def get_proxies_from_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        parser = fromstring(response.text)\n",
    "        proxies = list()\n",
    "        for i in parser.xpath('//tbody/tr')[:10]:\n",
    "            if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "                #Grabbing IP and corresponding PORT\n",
    "                proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "                proxies.append(proxy)\n",
    "        return proxies\n",
    "    \n",
    "    def get_proxies(self):\n",
    "        proxies = []\n",
    "        for proxies_source in self.proxies_sources:\n",
    "            proxies.extend(self.get_proxies_from_url(proxies_source))\n",
    "        return set(proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally just iterate over all tickers that we want to extract the underlying price (this is just the begining!).\n",
    "\n",
    "The code is self-explanatory, also I have added code to save the data got from the url to post-process it, for example looking for option prices or other information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY Underlying Price = 272.15 got with proxy: 192.116.142.153:8080 \n",
      "-------------------\n",
      "HPQ Underlying Price = 21.91 got with proxy: 128.199.254.244:3128 \n",
      "-------------------\n",
      "HPE Underlying Price = 15.52 got with proxy: 91.194.42.51:80 \n",
      "-------------------\n",
      "ILG Underlying Price = 34.18 got with proxy: 202.42.142.66:8080 \n",
      "-------------------\n",
      "ON Underlying Price = 25.85 got with proxy: 66.63.9.26:3128 \n",
      "-------------------\n",
      "T Underlying Price = 32.51 got with proxy: 185.93.3.123:8080 \n",
      "-------------------\n",
      "GE Underlying Price = 14.63 got with proxy: 35.227.54.242:3128 \n",
      "-------------------\n",
      "M Underlying Price = 34.13 got with proxy: 153.122.53.124:8118 \n",
      "-------------------\n",
      "Error: HTTPSConnectionPool(host='www.optionseducation.org', port=443): Max retries exceeded with url: /quotes.html?quote=BAC (Caused by ConnectTimeoutError(<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000006D75F28>, 'Connection to 152.157.119.253 timed out. (connect timeout=5)'))\n",
      "Skipping. Connnection error, server 152.157.119.253:3128\n",
      "-------------------\n",
      "BAC Underlying Price = 30.16 got with proxy: 192.116.142.153:8080 \n",
      "-------------------\n",
      "LEG Underlying Price = 41.63 got with proxy: 128.199.254.244:3128 \n",
      "-------------------\n",
      "-------END---------\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "import traceback\n",
    "from lxml.html import fromstring\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "tickers = ['SPY','HPQ', 'HPE', 'ILG', 'ON', 'T', 'GE', 'M', 'BAC', 'LEG']\n",
    "proxies = Get_proxies()\n",
    "user_agents = UserAgent()\n",
    "proxy_pool = cycle(proxies.get_proxies())\n",
    "for ticker in tickers:\n",
    "    url = 'https://www.optionseducation.org/quotes.html?quote=' + ticker\n",
    "    for i in proxy_pool:\n",
    "        proxy = next(proxy_pool) # Get a proxy from the pool\n",
    "        user_agent = user_agents.random   # Get a random user agent\n",
    "        headers = {'User-Agent': user_agent}\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            response = requests.get(url, proxies={\"http\": proxy, \"https\": proxy}, headers=headers, timeout = 5)\n",
    "            soup_oic = BeautifulSoup(response.text, \"lxml\")\n",
    "            with open(\"oic_page_\" + ticker + \".html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "                fp.write(response.text)\n",
    "            adress = \"\"\n",
    "            for iframe in soup_oic.find_all('iframe'):\n",
    "                if 'ivolatility' in iframe['src']:\n",
    "                    address = iframe['src']\n",
    "            req = requests.get(address, proxies={\"http\": proxy, \"https\": proxy}, headers=headers, timeout = 5)\n",
    "            soup = BeautifulSoup(req.text, \"lxml\")\n",
    "            with open(\"price_table_\" + ticker + \".html\",\"w\", encoding=\"UTF-8\") as fp:\n",
    "                fp.write(req.text)\n",
    "            ps = Process_soup()\n",
    "            print('{0} Underlying Price = {1} got with proxy: {2} '.format(ticker,\n",
    "                                                                           ps.get_underlying_price(soup),\n",
    "                                                                           proxy))\n",
    "            print(\"-------------------\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "            #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "            print(\"Error: {}\".format(e))\n",
    "            print(\"Skipping. Connnection error, server {}\".format(proxy))\n",
    "        print(\"-------------------\")\n",
    "print(\"-------END---------\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:web_scrapping3]",
   "language": "python",
   "name": "conda-env-web_scrapping3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
